# -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
#   
#   create_corpus.R
#   fridolin.wild@wu-wien.ac.at, August 1st 2006
#   
# -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  


tempdir = tempfile()

# directory structure

dir.create(tempdir)
dir.create(paste(tempdir,"corpora", sep="/"))
dir.create(paste(tempdir,"corpora/corpus.6",sep="/"))
dir.create(paste(tempdir,"corpora/corpus.6.base",sep="/"))

# scores

scores = paste(tempdir, "corpora/corpus.6.scores", sep="/")
write("data6_01.txt 3.00\ndata6_02.txt 2.75\ndata6_05.txt 2.75\ndata6_06.txt 3.00\ndata6_07.txt 3.75\ndata6_08.txt 1.25\ndata6_09.txt 2.25\ndata6_10.txt 4.00\ndata6_11.txt 3.00\ndata6_12.txt 3.75\ndata6_13.txt 2.00\ndata6_14.txt 2.75\ndata6_15.txt 3.00\ndata6_17.txt 3.00\ndata6_18.txt 3.00\ndata6_19.txt 2.75\ndata6_20.txt 2.75\ndata6_21.txt 3.00\ndata6_22.txt 3.00\ndata6_23.txt 3.50\ndata6_24.txt 2.75\ndata6_25.txt 3.00\ndata6_26.txt 4.00\ndata6_golden_01.txt 4.00\ndata6_golden_02.txt 4.00\ndata6_golden_03.txt 4.00", file=scores)

# test data

testdir = paste(tempdir, "corpora/corpus.6", sep="/")

write("Latent Semantic Analysis (LSA) ist eine automatische mathematische/statistische Technik um Relationen im Kontext von gebrauchten Wörtern zu extrahieren und abzuleiten. Um dies zu ermöglichen sind mehrere Schritte notwendig. Zuerst wird der Text als Matrix dargestellt. Dabei steht jede Reihe für ein einzelnes Wort und jede Spalte für einen Absatz. Jede Zelle enthält die Häufigkeit mit welchen das Wort vorkommt. Dann wird jede Zellhäufigkeit mit einer Funktion gewichtet. In dieser Funktion sind sowohl die Wortwichtigkeit im jeweiligen Absatz als auch das Ausmaß des Informationsgehaltes zum Gesamtinhalt enthalten. Im dritten Schritt findet eine Zerlegung der Matrix mittels einer Faktorenanalyse statt. Die rechteckige Matrix wird dabei in das Produkt von drei weiteren Matrizen zerlegt. Die erste Matrix repräsentiert die Originalreihe als Vektor der abgeleiteten Faktorwerte. Die zweite beschreibt die Originalspalte als Vektor der abgeleiteten Faktorwerte und die dritte ist eine diagonale Matrix welche die abgeleiteten Werte enthält. Multipliziert man die drei Matrizen so erhält man wieder die Originalmatrix.", file = paste( testdir, "data6_01.txt", sep="/" ) )
write("Im ersten Schritt wird der Text in eine Matrixform umgewandelt, in welcher einer jeden Zeile ein eindeutiges (unique) Wort zugeordnet wird und jede Spalte eine Textpassage beinhaltet. Die Werte in den einzelnen Zellen geben wiederum die Häufigkeit an, mit der das entsprechende Wort der gewählten Zeile in der betreffenden Textpassage der Spalte vorkommt. Als nächstes werden die Zellenwerte transformiert, indem in jeder Zelle die Häufigkeit mit einer Funktion, welche sowohl die Bedeutung des Wortes in der bestimmten Textpassage, als auch den Grad an Information, welche dieser Worttyp im Allgemeinen mitträgt, gewichtet wird. Im nächsten Schritt wendet LSA eine „Singular Value Decomposition“ (SVD) bei der Matrix an. Hier handelt es sich um eine Art der Faktor Analyse, welche - genauer ausgedrückt - eine Spezialform der „mathematische Generalisierung“ darstellt. Die Singular Value Decomposition spaltet nun die ursprüngliche rechteckige Matrix in das Produkt dreier neuen Matrizen auf. Mit Hilfe weiterer statistischer und mathematischer Transformationen kann somit nun jede Textpassage transformiert werden.", file = paste( testdir, "data6_02.txt", sep="/" ) )
write("LSA wird für Textanalysen benutzt. Das Ziel ist, inhaltliche Ähnlichkeiten zwischen zwei Texten zu erkennen. Mit dieser mathematischen/statistischen Methode kann man Rückschlüsse auf den erwarteten Inhalt von Texten ziehen. Diese Methode wir also verwendet um den Inhalt eines langen Texts zu extrahieren um so zu einem inhaltlich vergleichenden Ergebnis zu kommen. Die Schritte, die für diese Inhaltsanalyse durchgeführt werden müssen, sind folgende. Zuerst wird der Text in Matrixform zerlegt bzw aufgezeichnet, wobei jede Zeile für jedes einzelne Wort in ihr steht; jede Spalte steht für einen bestimmten Textteil. Damit kann man die Häufigkeit anzeigen mit der ein Wort in einer Zelle zeilen- oder spaltenweise im Text vorkommt. Durch diese Zellenbildung erfolgt dann eine Transformation. Darin wird zum Beispiel jede Häufigkeit der Wörter funktional gewichtet, d.h. es wird die Bedeutung eines Worts in einer bestimmten Textstelle untersucht, zum anderen untersucht man den Informationsgehalt eines Wortes selbst, diesmal aber ohne Kontextabhängigkeit. Im nächsten Schritt wird SVD (singular value decomposition) zur weiteren Analyse verwendet. Dabei geht die ursprüngliche, rechteckige Matrix in anderen drei Matrizen auf; die ursprüngliche Matrix wird also zerlegt. Die erste beschreibt die original Zeilenwerte, abgeleitet als ein Vektor rechtwinkeliger Faktorenwerte, die zweite beschreibt die original Spaltenwerte und die dritte ist eine diagonale Matrix, die aus der ersten und zweiten gebildet wird. Jede Matrix ist in ihre kleinste Dimension zerlegbar. Daraus läßt sich die Analyse durchführen.", file = paste( testdir, "data6_05.txt", sep="/" ) )
write("Grob wird die LSA in 4 Teilschritte unterteilt. 1 Schritt: Der zu untersuchende Text wird in eine Matrix übertragen. Für jedes Wort ist eine eigene Zeile gedacht, jede Reihe steht für eine Textpassage bzw. einen bestimmten Kontext. Nun wird für jedes Wort die Häufigkeit in die jeweilige Spalte (Passage) eingetragen. 2 Schritt: Nun muss die diese Matrix in eine „Einwert“-Matrix linear zerlegt werden. Bei dieser Art der Faktoranalyse wird die rechtwinkelige Matrix in 3 Matrixen aufgespalten. Die erste Matrix beschreibt die mit orthogonalen Faktorwerten (Relationen zu den anliegenden Werten) gewichtete/abgeleitete Matrix bezogen auf 2 Verhältnisse in den Reihen. Bei der zweiten Matrix wird dasselbe mit den Spalten gemacht. Nun haben alle Wörter einen Wert egal ob sie tatsächlich im Satz enthalten waren oder nicht. Die dritte Matrix ist eine Diagonalmatrix mit deren Hilfe man die ursprünglichen Werte der ersten beiden Matrixen wieder rekonstruieren kann. 3 Schritt: Hier wird die ursprüngliche 2 Dimensionen Matrix wieder hergestellt, jedoch nur mit Hilfe der Vektorenelemente zweier Referenzzeilen. Auch wenn also ursprünglich ein Wort in einer Zeile gar nicht vorgekommen ist, hat die Zelle einen Wert, der stark davon abhängt, in welchem Zusammenhang er mit den anliegenden Zellen stehen mag. Umso höher der Wert umso wichtiger scheint dessen Bedeutung im Kontext zu sein. Dieser Faktorwert kann jedoch nur eine Schätzung sein, wie oft ein Wort X im Zusammenhang Y vorkommt. 4 Schritt: Damit diese Zahlen also eine positive oder negative Korrelationen zwischen den einzelnen Zeilen des Textes mit den anderen Zeilen ausdrücken, muss man diese einmal anhand der Rohdaten berechnen und einmal mit den Vektoren der 2 dimensionalen Matrix.", file = paste( testdir, "data6_06.txt", sep="/" ) )
write("Latent Semantik Analysis (LSA) ist ein Verfahren, das zur vollautomatischen Indizierung von Texten und zum Textretrieval entwickelt wurde. Andere Verfahren funktionieren entweder sehr einfach (simples Wortmatchen) und liefern dementsprechend ungenaue Ergebnisse. Linguistische Verfahren sind zwar anspruchsvoller und versprechen dementsprechend bessere Ergebnisse, verlangen dafür aber oft Handarbeit und hohen rechnerischen Aufwand. Bekannteste Probleme sind Synonymen und Homographen, außerdem sind Wortabhängigkeiten ein Problem. Im Folgenden werden der Formalismus dargestellt, wie man die Ähnlichkeit von zwei Texten untersuchen kann. Die Lösung ist es, Dokumente und Worte werden als Vektoren im gleichen multidimensionalen Vektorraum darzustellen. Über einen Winkelvergleich der Vektoren untereinander kann die Ähnlichkeit festgestellt werden. Der erste Schritt ist, den Text als eine Matrix darzustellen, in der jede Reihe für ein einzelnes Wort steht und jede Spalte für eine Textpassage. Jede Zelle zeigt die Häufigkeit an, mit der das Wort jeder Reihe in der Textpassage erscheint. Danach werden die Zelleinträge umgeformt. Ein wichtiger Teil der Analyse ist die Vorverarbeitung der Daten. Dabei werden die Daten in eine Rohform gebracht, nämlich werden die Wörter je nach Kontext in der Matrix umgewandelt. Zuerst wird die Häufigkeit der Wörter festgehalten. Danach wird die Entropie gemessen. Das Ergebnis dieser Umformung ist, jede Anzahl des Worttyps mit der Wichtigkeit in der Textpassage zu verbinden. Im nächsten Schritt wird die Singular Value Decomposition (SVD) auf die Matrix angewendet. Dies ist eine Art von Faktoranalyse, genauer gesagt die mathematische Generalisierung, eine Art der Faktoranalyse. Bei SVD wird die rechteckige Matrix in drei Matrizen aufgespaltet. Eine Matrix beschreibt die ursprünglichen Reihenentitäten als Vektoren von abgeleiteten Faktorwerten. Eine andere die ursprünglichen Spaltenentitäten in derselben Art. Die dritte ist eine diagonale Matrix, die skalierte Werte beinhaltet, sodass wenn die drei miteinander multipliziert werden, wieder die originale Matrix.", file = paste( testdir, "data6_07.txt", sep="/" ) )
write("Im Text werden 9 Titel miteinander mittels LSA verglichen. Folgende Schritte werden dafür gesetzt. Als erstes wird eine Matrix erstellt, in der die Wort – zu – Wort – Bezüge dargestellt werden. Im nächsten Schritt kommt die singuläre Wertauflösung zum Zug. Hierbei wird die Matrix vom ersten Schritt in drei andere Matrizen aufgeteilt. Das Produkt dieser drei Matrizen ist wiederum die erste Matrix. In zwei dieser Matrizen werden die ursprünglichen Reihenentitäten als Vektoren von den abgeleiteten Werten dargestellt. Die dritte Matrix die aus der „Ur-Matrix“ folgt ist eine diagonale Matrix, die multipliziert mit den anderen zwei Matrizen wiederum die Ur-Matrix ergibt. Bei der Ableitung von der Ur-Matrix kommt es beim LSA-Verfahren jedoch zu Rundungsfehlern.", file = paste( testdir, "data6_08.txt", sep="/" ) )
write("Der Titel LSA (Latent Semantic Analysis) deutet bereits darauf hin, dass es sich um eine Methode handelt, die in der Lage ist, die latente semantische Struktur zwischen Wörtern aufzudecken. Latente Semantik bedeutet verborgene Information. Es ist eine Methode, die durch mathematische/statistische Techniken, sich folgernde, zusammenhangsbezogene Wörter in Textpassagen finden soll. Der erste Schritt ist einen Text in einer Matrix  darzustellen. Hierbei gilt, dass jede einzelne Reihe für ein bestimmtes Wort und jede Spalte für eine Textpassage reserviert ist. Der nächste Schritt ist die Singulärwertzerlegung (SVD). Dabei wird eine rechteckige Matrix, in drei andere Matrizen zerlegt. Dahergehend, dass man viele Wörter in die Matrix miteinbezieht, wird diese auch sehr groß ausfallen. Die erste Matrix beschreibt die Wörter, die zweite die Passagen und die dritte ist eine Diagonalmatrix. Da beim Multiplizieren von Matrizen Produkte der Einträge addiert werden, und die Reihenfolge bei der Addition egal ist, können Zeilen und Spalten der Diagonalmatrix vertauscht werden, solange auch die entsprechenden Spalten von den beiden anderen Matrizen vertauscht werden. Dadurch kann erreicht werden, dass die Werte auf der Diagonalmatrix, welche Singulärwerte heißen, absteigend geordnet sind. Dadurch kommt man zu kleineren Faktoren, die nicht berücksichtigt werden müssen. Durch dieses Verfahren findet eine Annäherung statt, welche man auch als Dimensionsreduktion bezeichnen kann, wobei man hofft, dass die wichtigen Wörter übrigbleiben. Die Methode liefert ein Maß für die Ähnlichkeit von Wörtern.", file = paste( testdir, "data6_09.txt", sep="/" ) )
write("LSA ist eine Methode um semantisch äquivalente, aber unterschiedlich geschriebene Wörter mit Hilfe einer mathematischen Methode zu identifizieren. In einem ersten Schritt wird eine „Textmatrix“ (Dokument/Wort-Matrix) erstellt, welche in den Zeilen jedes der einzelnen Worte  der zu prüfenden Texte enthält und in den Spalten die Textpassage, Dokument bzw. den Kontext. Jede Zelle enthält vorerst die Anzahl (frequency) mit der es in dem entsprechenden zugeordneten Text auftritt. Weiters wird für jedes Wort ein gewichteter Wert berechnet und zwar mit Hilfe einer Funktion, die sowohl die Relevanz des Wortes in dem Dokument als auch den Grad des Informationsgehalts des Wortes im allgemeinen enthält. Als nächster Schritt wird die Singular Value Decomposition (SVD) durchgeführt. Diese Methode ist eine Form der Faktor-Analyse, mit Hilfe derer die Wort/Dokument-Matrix (X) in drei andere Matrizen gespalten wird, nämlich die orthogonalen Matrizen W und P und in die Diagonale S. Eine Matrize beschreibt die Original-Zeileneinträge als Vektoren der orthogonalen Faktorwerte, die zweite die Spalteneinträge auf dieselbe Art und Weise. Die dritte, die Diagonale enthält die Skalenwerte (singular values) die durch Multiplikation wieder die Original-Matrixwerte ergeben. Nach Neuordnung dieser Werte und Umsortierung der ursprünglichen Matrizen wird die ursprüngliche Matrize X in einen geringer dimensionalen Raum durch das Nullsetzen aller Werte außer dem höchsten projeziert. Die Ähnlichkeit zweier Worte wird durch die Position der Wort-Vektoren in der Matrize im reduziert-dimensionalen Raum bestimmt. (z.B.: durch den Winkel zwischen den Vektoren).", file = paste( testdir, "data6_10.txt", sep="/" ) )
write("Formalismus: 1. Den Text in Form einer Matrix darstellen. Zeilen stehen für ein einzelnes Wort. Spalten für eine Texteinheit. Zellen beinhalten die Häufigkeit mit der das Wort in der Texteinheit vorkommt. 2. Häufigkeit wird mit bestimmter Funktion gewichtet. 3. Die erstellte Matrix X wird in drei Faktoren (= drei Matritzen) aufgespalten, und zwar durch eine Operation, die Singular Value Decomposition (SVD) heißt ( X = W0 S0 D0T). Die Matritzen enthalten linear unabhängige Vektoren und ergeben multipliziert miteinander natürlich wieder die Ausgangsmatrix. Für die drei Faktoren gilt. W0 ist die Matrix der Eigenvektoren der quadratischen Matrix Y1 = XXT. D0 ist die Matrix der Eigenvektoren der quadratischen Matrix Y2 = XTX. S0 ist die Matrix der Eigenwerte der Eigenwert-Eigenvektor-Dekomposition. Nun wird noch die Dimensionalität der Matrix reduziert. Angefangen bei den kleinsten Werten in der S0 Matrix", file = paste( testdir, "data6_11.txt", sep="/" ) )
write("LSA ist eine vollautomatische mathematisch-statistische Methode zur Identifizierung semantisch gleichwertiger, aber verschieden geschriebener Wörter. Zunächst wird der zu untersuchende Text als Matrix dargestellt, in der jede Reihe für ein einzigartiges Wort und jede Spalte für eine Textpassage oder den Kontext steht. Jede Zelle enthält nun die Häufigkeit – frequency – eines Wortes einer Reihe mit der es in der durch die Spalten repräsentierten Textpassage vorkommt. In Folge wird die Zellenhäufigkeit mittels einer Funktion gewichtet, die die Relevanz und den Informationsgehalt des Wortes veranschaulicht. Die Wort-Text-Matrix wird in einem weiteren Verfahrensteil mit Hilfe der „Singular Value Decomposition“ (SVD) – Methode, einer Form der Faktor-Analyse, in drei neue Matrizen gespalten. Diese Matrizen enthalten als Vektoren dargestellt die eigentlichen Zeilen-Spalteneinträge und die „singular values“ – die Skalenwerte. Darauf folgend wird die Ausgangsmatrix mit Hilfe des Nullsetzens aller Werte, mit Ausnahme des Höchsten in einen kleiner-dimensionalen Raum projiziert. Vorab werden die Werte noch neu geordnet und neu sortiert. Schließlich lässt die Stellung der Vektoren im kleindimensionalen Raum Rückschlüsse auf die Ähnlichkeit der Ursprungsinhalte zu.", file = paste( testdir, "data6_12.txt", sep="/" ) )
write("LSA ist ein Verfahren, das zur vollautomatischen Indizierung von Texten und zum Text-retrieval entwickelt worden. Um eine solche Indizierung durchzuführen müssen erstmals Dokumente und Worte als Vektoren dargestellt werden. Diese müssen sich allerdings in einem gleichen multidimensionalen Vektorraum befinden. Das Ziel liegt darin semantisch äquivalente, aber unterschiedlich geschriebene Wörter zu identifizieren. Kurz beschrieben kann dieses Verfahren in 3 Schritten durchgeführt werden. Der Zielsetzung, die Eingabe (Texte oder Textphrasen werden in einer einheitlichen Sprache eingegeben) und schlussendlich das Ergebnis: Eine Auflistung von Wörtern die zu einem Begriff semantisch äquivalent sein könnten. Genauer betrachtet werden die Daten, zur Datenmengenreduktion, in einem mathematischen  Verfahren bearbeitet. Dazu werden diese Daten in Form einer Matrix repräsentiert, und durch das System der Singular Value Decomposition (SVD) faktorisiert. Die zentrale Datenstruktur ist also die Matrix wobei Terme hierbei Worte sind, welche in mindestens zwei (bis ca. 1000-2000) Dokumenten vorkommen. Ein Element in dieser Matrix repräsentiert die Häufigkeit eines Terms innerhalb eines Dokuments. Anschließend wird die erstellte Matrix mittels SVD in drei weitere Matrizen mit linear unabhängigen Vektoren aufgespalten. 1) W0: Matrix der Eigenvektoren der quadratischen Matrix XXT. 2) D0:  Matrix der Eigenvektoren der quadratischen Matrix XTX. 3) S0:  Matrix der Eigenwerte der Eigenwert-Eigenvektor-Dekomposition. Die Zeilen und Spalten der Matrix S0 werden so vertauscht, dass die Werte der Größe nach geordnet auf der Mitteldiagonale liegen. Als letzter Schritt folgt die Reduzierung  der Dimensionalität der Matrix S0  auf S (gleichzeitige Reduzierung von Matrix W0 auf W und D0 auf D). Multipliziert man zu guter letzt die drei Matrizen S, W und D miteinander, kann man damit die im Text verborgenen semantischen Strukturen aufdecken. ", file = paste( testdir, "data6_13.txt", sep="/" ) )
write("Der erste Schritt ist, den Text wie eine Matrix darzustellen, in der jede Reihe für ein einzigartiges Wort und jede Spalte für eine Textpassage oder für anderen Kontext stehen. Jede Zelle enthält die Frequenz, mit der das Wort seiner Reihe in der Passage erscheint, die durch seine Spalte kennzeichnet wird. Zunächst werden die Zelleintragungen einer einleitenden Umwandlung unterworfen, in der jede Zelle Frequenz durch eine Funktion gewichtet wird, die sowohl den Wichtigkeit des Wortes in der bestimmten Passage als auch den Grad ausdrückt, für den das Worttypus im Allgemeinen Informationen in dem Diskursbereich trägt. Danach wendet LSA singuläre Wertaufspaltung (SVD) an der Matrix an. Diese SVD ist eine Form der Faktoranalyse. In SVD wird eine rechteckige Matrix in das Produkt von drei anderen Matrizen zerlegt. Eine Teilmatrix beschreibt die ursprünglichen Reihenentitäten als Vektoren der abgeleiteten orthogonalen Werte, eine andere beschreibt die ursprünglichen Spaltenentitäten in gleicher Art und Weise, und die dritte ist eine diagonale Matrix, die solche Skalenwerte enthält, durch die so, wenn die drei Teile matrix-multipliziert sind, die ursprüngliche Matrix wieder rekonstruiert wird.", file = paste( testdir, "data6_14.txt", sep="/" ) )
write("Erster Schritt: Abbildung eines Textes in Form einer Matrix, wobei jede Reihe für ein Wort und jede Spalte für eine Textpassage steht. Jeder Wert (jede Zelle) gibt an, wie oft ein bestimmtes Wort in einer bestimmten Textpassage vorkommt. Zweiter Schritt: Die Werte werden gewichtet. Der Algorithmus beachtet die Wichtigkeit eines bestimmten Wortes in einer bestimmten Textpassage sowie die Wichtigkeit dieses Wortes für dieses Thema im allgemeinen. Dritter Schritt: SVD (Single Value Decomposition): SVD ist die Generalisierung der Faktoranalyse. Eine Matrix wird in drei Matrizen verwandelt, deren Produkt die ursprüngliche Matrix ist. Vierter Schritt: Die Dimensionen werden reduziert, indem Koeffizienten der diagonalen Matrix (siehe Figur 2, Seite 11) gelöscht werden. Fünfter Schritt: Rekonstruktion der Matrix mit reduzierten Dimensionen.", file = paste( testdir, "data6_15.txt", sep="/" ) )
write("Mit Hilfe von LSA kann die Ähnlichkeit 2er Texte gemessen werden. Der zu analysierende Text wird in eine Tabelle übertragen und aufgegliedert. In den Zeilen werden die semantischen Wörter und in den Spalten die exakte Position im Text festgehalten. Nun wird, wie bei vielen Scoring Methoden, ein gewichteter Wert für jedes Wort berechnet. Neben der Gewichtung ist auch die Häufigkeit des Vorkommens im Text in jeder Zelle vermerkt. Durch eine mathematische Formel wird der Grad der Gewichtung berechnet, wobei folgende Punkte relevant sind: die Aussagekraft eines Wortes im Dokument sowie die Aussagekraft des Wortes im Allgemeinen. Mit Hilfe der Singular Value Decomposition wird der Text aufgegliedert und eine Faktor Analyse durchgeführt. Das nun aus 3 Tabellen bestehende Dokument enhält die Tabelle W und P sowie eine Diagonale S. Durch die Multiplikation der Tabellen W und P, der Umsortierung und Neuordnung aller Skalenwerte sowie dem Nullsetzen aller Werte wird nun die Ähnlichkeit auf Grund der Nähe zueinander deutlich.", file = paste( testdir, "data6_17.txt", sep="/" ) )
write("Latent Semantic Analysis (LSA) ist ein Verfahren, das zur automatischen Indizierung von Texten und zum Textretrieval entwickelt wurde. Im Folgenden werden die einzelnen Schritte dargestellt, wie man die Ähnlichkeit von zwei Texten untersuchen kann. Der erste Schritt ist, den Text als eine Matrix darzustellen, in der jede Reihe für ein einzelnes Wort steht und jede Spalte für eine Textpassage. Jede Zelle zeigt die Häufigkeit an, mit der das Wort jeder Reihe in der Textpassage erscheint. Danach werden die Zelleinträge umgeformt. Ein wichtiger Teil der Analyse ist die Vorverarbeitung der Daten. Dabei werden die Daten in eine Rohform gebracht, nämlich werden die Wörter je nach Kontext in der Matrix umgewandelt. Zuerst wird die Häufigkeit der Wörter festgehalten. Danach wird die Entropie (Mass für den Informationsgehalt) gemessen. Das Ergebnis dieser Umformung ist, jede Anzahl des Worttyps mit der Wichtigkeit in der Textpassage zu verbinden. Im nächsten Schritt wird die Singular Value Decomposition (SVD) auf die Matrix angewendet. Dies ist eine Art von Faktoranalyse, genauer gesagt die mathematische Generalisierung, eine Art der Faktoranalyse. Bei SVD wird die rechteckige Matrix in drei Matrizen aufgespalten. Eine Matrix beschreibt die ursprünglichen Reihenentitäten als Vektoren von abgeleiteten Faktorwerten. Eine andere die ursprünglichen Spaltenentitäten in derselben Art. Die dritte ist eine diagonale Matrix, die skalierte Werte beinhaltet, sodass wenn die drei miteinander multipliziert werden, wieder die originale Matrix herauskommt.", file = paste( testdir, "data6_18.txt", sep="/" ) )
write("Zunächst wird aus dem Text eine Matrix gebildet. Jede Zeile steht für ein bestimmtes Wort und jede Spalte steht für eine Textpassage bzw. einen Sinnzusammenhang. Die Zellen beinhalten die Häufigkeit mit der das jeweilige Wort aus der Zeile in der Textpassage der jeweiligen Spalte vorkommt. Danach werden die Zelleinträge einer vorläufigen Transformation unterzogen. Dabei werden die Zellhäufigkeiten mit einer Funktion gewichtet, welche die Wichtigkeit des jeweiligen Wortes in einer bestimmten Passage ausdrückt. Als nächstes wird eine Art von Faktoranalyse durchgeführt. Das Verfahren nennt sich singular value decomposition (SVD). Dabei wird eine rechteckige Matrix in drei andere Matrizen zerlegt. Eine Teilmatrix beschreibt die ursprünglichen Zeileneinträge als Vektoren der abgeleiteten rechtwinkeligen Faktorwerte. Die zweite Teilmatrix erfüllt im Prinzip dieselbe Aufgabe jedoch bezieht sich diese auf die ursprünglichen Spalteneinträge. Die dritte Teilmatrix ist eine diagonale Matrix, welche Skalierungswerte enthält, um beim Multiplizieren der drei Matrizen die ursprüngliche rechteckige Matrix wiederherzustellen.", file = paste( testdir, "data6_19.txt", sep="/" ) )
write("LSA ist ein automatisches, mathematisch-statistisches Verfahren, um die Ähnlichkeit von zwei Texten zu untersuchen. Dafür wird im 1. Schritt der Text als Matrix dargestellt, bei dem jede Zeile für ein einmaliges Wort steht und jede Spalte für eine Textpassage. Jede Zelle beinhaltet die Häufigkeit mit dem ein Wort einer Zeile in der Textpassage vorkommt. Danach werden die Zelleneinträge transformiert und durch eine Funktion gewichtet. Dadurch erhält man eine Angabe, die die Wichtigkeit und den Grad des Informationsgehalts enthält. Als nächstes bringt man ein s.g. SVD, das steht für singular value decomposition, an die Matrix an. Dabei handelt es sich um eine Art der Faktoranalyse. Die rechtwinkelige Matrix wird hierbei das Produkt von 3 anderen Matrizen. Eine Matrix beschreibt dabei die ursprünglichen Zeileneinträge als Vektoren der abgeleiteten orthogonalen Faktorwerte, die Zweite beschreibt die ursprünglichen Spalteneinträge und die Dritte ist eine diagonale Matrix. ", file = paste( testdir, "data6_20.txt", sep="/" ) )
write("Der erste Schritt ist, den Text als Matrix darzustellen, in der jede Reihe für eindeutiges Wort  und jede Spalte für einen Textstelle oder anderen Kontext steht.  Jede Zelle enthält die Frequenz, mit der die Wortreihe in der Stelle der Spalte aufscheint.  Zunächst werden die Zelleneinträge einer einleitenden Transformation unterworfen, in denen jede Zellenfrequenz durch eine Funktion gewichtet wird. Diese drückt  Bedeutung des Wortes in der bestimmten Stelle und den Grad, wie die Wortart die Informationen trägt, aus. Zunächst wendet LSA einzigartige Wertaufspaltung (SVD) an der Matrix an.  Dieses ist eine Form der Faktoranalyse. In SVD wird eine rechteckige Matrix in das Produkt von drei anderen Matrizen zerlegt.  Eine Teilmatrix beschreibt die ursprünglichen Reiheninstanzen als Vektoren vom berechneten orthogonalen Faktorwert. Andere beschreiben die ursprünglichen Spalteinstanzen in der gleichen Art und Weise. Der Dritte ist eine diagonale Matrix, enthält Skalenwerte, sodass wenn die drei Bestandteile Matrix - multipliziert werden,  wird die ursprüngliche Matrix wieder aufgebaut. ", file = paste( testdir, "data6_21.txt", sep="/" ) )
write("Der erste Schritt ist, den Text in die Form einer Matrix überzuleiten, bei der jede Reihe für ein einzelnes Wort und jede Spalte für eine Text- Passage oder anderen Kontext steht. Die mathematischen Operationen, die dem ganzen zugrunde liegen, bestehen aus einer zweiteiligen Transformation. Zuerst wird die Wort-Frequenz (+1) in jeder Zelle logarithmiert. Danach wird der „Informationsgehalt“ (Entropie) mittels der Operation – p log p für alle Daten einer Zeile berechnet und jeder Zell- Eintrag anhand des Zeilen- Entropie- Wertes geteilt. Als nächstes wird die „Singular Value Decomposition“ (SVD), eine Form der Faktor-Analyse, auf die Matrix angewandt.  Bei der SVD wird die Ausgangsmatrix in drei andere Matrizen zerlegt, wobei eine Teil-Matrix die ursprünglichen Zeilen-Entitäten als Vektoren der abgeleiteten orthogonalen Faktor- Werte enthält, eine zweite beschreibt die originalen Spalten- Entitäten auf die gleiche Art und Weise, und die dritte ist eine diagonale Matrix, die derart skalierte Werte enthält, dass bei einer Matrizen-Multiplikation aller drei Teil-Matrizen die ursprüngliche Ausgangs-Matrix wiederhergestellt wird.", file = paste( testdir, "data6_22.txt", sep="/" ) )
write("LSA (Latent Semantic Analysis) ist ein mathematische Technik zum Aufsuchen vom zusammenhängenden Wörtern oder Texten. Der erste Schritt ist, dass der Text in einer Matrix gezeigt wird. In jeder Zeile wird das Wort und in jeder Spalte die Textpassage abgebildet. In jeder Zelle ist die Häufigkeit ersichtlich, wie oft ein Wort in der Textpassage vorkommt. Danach werden die Einträge einer Vorauswahl unterworfen, wobei die Häufigkeiten mit einer Funktion gewichtet wird, die die Wichtigkeit des Wortes in der Passage angibt und wie hoch der Informationsgrad dieses Wortes ist. Im zweiten Schritt wird die Ausgangsmatrix in drei Matrizen aufgespaltet. Dabei werden nicht alle Faktoren der drei Matrizen verwendet. Die verwendeten Faktoren werden durch lineare Kombinationen verbunden. Diese Aufspaltung kann aber nicht fehlerfrei erfolgen. Es kann passieren, dass Wörter mit einer abweichenden Häufigkeit gezeigt werden oder dass Häufigkeiten vorkommen, die vorher noch nicht da waren. Zum Überprüfen, was bei dieser Zerlegung passiert, werden Inter-Korrelationen zwischen den Titel herangezogen. Eine weitere Matrix mit 2 Dimensionen auf Basis der ursprünglichen Matrix bewirkt, dass die Gruppierung der Themen treffender wird.", file = paste( testdir, "data6_23.txt", sep="/" ) )
write("Latent Semantic Anlaysis (LSA) ist eine völlig automatisierte mathematische/statistische Methode um Beziehungen bzw. Verbindungen zwischen Wörtern abzuleiten. LSA verwendet keine Grammatik, semantische Netzwerke, selbst aufgebaute Wörterbücher, sondern verwendet nur Rohtexte, analysiert diese und verwandelt sie in sinnvolle Textpassagen oder Sätze. Nachfolgend wird erläutert, welche Schritte durchgeführt werden müssen, um mit Hilfe von LSA die Ähnlichkeit von zwei Texten zu untersuchen. Als Erstes wird der Text als eine Matrix dargestellt, wobei jede Reihe für ein einzelnes Wort und jede Spalte für eine Textpassage steht. Jede Zelle beinhaltet die Häufigkeit, mit der das Wort jeder Reihe in der Textpassage erscheint. Danach werden die Zelleinträge transformiert, wobei jede Zelle nach 2 Merkmalen in einer Passage gewichtet wird: Wichtigkeit des Wortes, Grad zu welchem der Worttyp, Informationen überträgt. Als nächstes wird die Singular Value Decomposition (SVD) auf die Matrix angewendet. Dies ist eine Art Faktoranalyse. In der SVD wird die rechtwinkelige Matrix in die Produkte drei anderer Matrizen aufgeteilt. Die erste Matrix beschreibt die ursprüngliche Zeile als Vektor abgeleiteter rechtwinkeliger Faktorwerte. Die zweite Matrix beschreibt die ursprüngliche Spalte derselben Art und Weise. Die dritte Matrix ist eine diagonale Matrix die skalierbare Werte beinhaltet. Wenn alle drei Matrizen miteinander multipliziert werden, gelangt man wieder zur ursprünglichen Matrix.", file = paste( testdir, "data6_24.txt", sep="/" ) )
write("Mit Latent Semantic Analyses (LSA) können erwartete Beziehungen von Wort und Textpassagen extrahiert werden. Es handelt sich dabei um ein vollautomatisches mathematisch – statistisches Verfahren. Zunächst wird in eine Matrix umgewandelt, wobei jede Spalte für eine Textpassage steht und jede Zeile für ein Wort. In den Zellen ist die Häufigkeit der Worte eingetragen. Jetzt werden die Inhalte der Zellen gewichtet (mithilfe einer Funktion) und somit festgelegt, wie wichtig und informativ das jeweilige Wort ist. Im nächsten Schritt wird mit der „Single value decomposition“ die Matrix in drei Matrizen zerlegt. Eine Matrize beschreibt die Zeilen, eine die Spalten und eine ist die Diagonalmatrize. Da keine Matrix perfekt zerlegt werden kann, wird auch eine Matrix erzeugt, die die Ausgangsmatrix auf zwei Dimensionen rekonstruiert. Damit wird erreicht, dass sich gleiche Beziehungen anpassen und die Gruppierung wird eindeutiger.", file = paste( testdir, "data6_25.txt", sep="/" ) )
write("Latent Semantic Analyses (LSA) ist ein vollautomatisches mathematisch-statistisches Verfahren, dass erwartete Beziehungen von Wort oder Textpassagen extrahiert. Im ersten Schritt wird der Text als Matrix dargestellt. Jede Zeile steht für ein Wort und jede Spalte für eine Textpassage. Die Zellen zeigen an, wie oft das Wort in den Textpassagen vorkommt. Die Zelleneinträge werden mit einer Funktion gewichtet, die angibt, wie wichtig das Wort in der jeweiligen Passage ist und welchen Grad an Informationsgehalt dieses Wort im Allgemeinen hat. Danach wird mithilfe der „Single value decomposition“ die Ausgangsmatrix in drei Matrizen zerlegt. Eine Matrix beschreibt die Zeilen, eine die Spalten als Vektoren und die dritte ist eine Diagonalmatrix. Es ist mathematisch bewiesen, dass keine Matrix perfekt zerlegt werden kann und die Häufigkeiten der zerlegten Matrizen nicht mit der Ausgangsmatrix zusammen passen. Es wird auch eine Matrix erzeugt, die die Ausgangsmatrix auf 2 Dimensionen rekonstruiert. Dieser Vorgang bewirkt, dass sich gleiche Beziehungen gegenseitig anpassen. Dadurch wird die Gruppierung der Bereiche eindeutiger. Die Verbesserung zwischen der Zerlegung und der Rekonstruktion wird mit Hilfe von Korrelation zwischen einem Titel und alle anderen gemessen.", file = paste( testdir, "data6_26.txt", sep="/" ) )
write("Schritt 1: Repräsentation der Texte in einer Matrix. Die Zeilen der Matrix stellen die einzelnen Wörtern und die Spalten einen Absatz oder eine andere Zusammenhang dar. Jede Zelle beinhaltet die Häufigkeit mit welcher ein Wort  seiner Zeile im von seiner Spalte angegebenen Absatz vorkommt. Die Zelleneinträge unterliegen eine Vortransformation, in welchen die Häufigkeit der Zellen durch eine Funktion gewichtet ist. Diese Funktion drückt gleichzeitig die Wichtigkeit des Wortes in den einzelnen Absätzen sowie den Grad, zu welchem das Worttyp Informationen im Allgemeinen enthalten ist, aus. Schritt 2: Nächstens werden die Daten mit einem mathematischen Verfahren (Faktorisierung durch Singular Value Decomposition) bearbeitet, wobei die Datenmenge reduziert wird und wodurch die in den Texten verborgene semantische Struktur aufgedeckt werden soll. Durch Zerlegung und Umwandlung wird die optimale Dimension erreicht, welche die korrekte Induktion der zugrundeliegende Relationen bestimmt. Schritt 3: Dokumente und Worte werden als Vektoren im gleichen multidimensionalen Vektorraum dargestellt. Der Kosinus zwischen den Vektoren ist üblicherweise (aber nicht immer) der Maß der Ähnlichkeiten zwischen der zwei Texten.", file = paste( testdir, "data6_golden_01.txt", sep="/" ) )
write("LSA ist eine Methode, die versucht Kontext-Zusammenhänge von unterschiedlichen Texten zu finden und zu beschreiben. Die Technik geht nach folgenden Schritten vor. Zu Beginn wird der zu untersuchende Text als Matrix dargestellt, wobei jede Reihe ein Wort, und jede Spalte eine Textpassage repräsentiert. Im nächsten Schritt werden die einzelnen Zellen transformiert. Dabei wird die Zellenhäufigkeit mit einer Funktion gewichtet die einerseits die Relevanz eines Wortes in der jeweiligen Textpassage und andererseits den generellen Informationsgehalt des Wortes widerspiegelt. Danach wird eine \"Einzelwertzerlegung\", eine Form der Faktoranalyse, an der Matrix angewandt. Dabei wird die rechteckige Matrix in drei neue Matrizen zerlegt. Die erste der drei Komponentenmatrizen beschreibt die Zeilenwerte der Originalmatrix als Vektoren des rechteckigen Faktorwertes, die zweite beschreibt die originalen Spaltenwerte und die dritte ist eine Diagonalmatrix, welche die abgestuften Werte enthält. Die Originalmatrix ist das Produkt der drei Matrizen. Durch Multiplizieren der drei neuen Matrizen kann also die Originalmatrix wiederhergestellt werden. Werden nicht alle Faktoren verwendet, zeigt die wiederhergestellte Matrix die beste Passung des kleinsten Quadrats (least-squares best fit). Die Lösung kann ganz einfach reduziert werden, indem Faktoren in der Diagonalmatrix gelöscht werden. Üblicherweise wird dabei mit dem kleinsten Faktor begonnen. Um die Auswirkung der Reduktion von Dimensionen auf die Relation zwischen den Faktoren zu untersuchen, wird die Interkorrelation zwischen den einzelnen Faktoren berechnet. Aufgrund dieser Interkorrelationen kann auf die Kontextzusammenhänge der einzelnen Faktoren geschlossen werden.", file = paste( testdir, "data6_golden_02.txt", sep="/" ) )
write("Mit Hilfe der Latent Semantic Analysis wird versucht, semantisch äquivalente, aber unterschiedlich geschriebene Worte zu identifizieren, und zwar durch mathematische Methoden. Zuerst wird vom vorgegebenen Text eine Dokument/Wort Matrix erstellt, in die jedes einzelne Wort der Textpassage in Zusammenhang mit der Häufigkeit des Auftretens innerhalb des Textes/der Textstelle gebracht wird. Es wird weiters ein Wert errechnet, der sowohl nach der Relevanz des Wortes in dem untersuchten Abschnitt gewichtet ist, als auch nach dem Grad des Informationsgehaltes unabhängig von dem Text. Durch die folgende Singular Value Decomposition (SVD) wird eine Faktoranalyse durchgeführt und die Dokument/Wort Matrix in drei Matrizen auf gespalten, die erstens die originalen Zeileneinträge als Vektoren der orthogonalen Faktorwerte, zweitens die Spalteneinträge auf selbige Art und drittens (Diagonale) die singular values, die Skalenwerte enthalten, welche durch Multiplikation wieder die originären Matrixwerte ergeben. Nun werden die Werte neu sortiert und ausgerichtet, die Faktoren werden reduziert, sodass die Matrix in einem weniger dimensionalen Raum projezierbar ist. Nun können zwei Worte verglichen werden, indem die Entfernung der jeweiligen Wortvektoren innerhalb der reduzierten Matrix ermittelt wird und man kann auf diese Weise Rückschlüsse auf die semantische Bedeutung der Worte ziehen.", file = paste( testdir, "data6_golden_03.txt", sep="/" ) )

# training data

trainingsdir = paste(tempdir, "corpora/corpus.6.base", sep="/")

write("Es wird ein Verfahren, genannt LSI mit SVD, motiviert und beschrieben, welches zu einer sehr speziellen Vektordarstellung von Dokumenten und insbesondere Anfragen führt.", file = paste( trainingsdir, "data6_base_01.txt", sep="/" ) )
write("Die Arbeit gründet sich auf dem Artikel Indexing by Latent Semantic Analysis von Scott Deerwester und anderen, der 1990 im Journal of the American Society of Information Science erschien. Darin wird eine neue Methode für automatisches Indexieren und Retrieval beschrieben, dass vor allem die durch Synonyme verursachten schlechten Recall-Werte des Retrievals zu verbessern versucht.", file = paste( trainingsdir, "data6_base_02.txt", sep="/" ) )
write("Wie schon aus dem Titel von Deerwesters Artikel hervorgeht, gründet sich der Ansatz zu dieser Methode auf der Annahme, dass die in den Dokumenten verwendeten Terme eine „semantische Struktur“ höherer Ordnung besitzen. Grob gesagt, gibt es Abhängigkeiten und Korrelationen zwischen den Indextermen. Durch Ausnutzen dieser Struktur beim Indexieren kann das Auffinden relevanter Dokumente mittels Suchtermen verbessert werden.", file = paste( trainingsdir, "data6_base_03.txt", sep="/" ) )
write("Das konkrete Verfahren zu diesem „Latent Semantic indexing“ (LSI) ist die sogenannte „Singular Value Decomposition“ (SVD), zu deutsch Singulärwertzerlegung. Dieses mathema¬tische Verfahren ermöglicht es, eine Matrix (in unserem Fall die Term-Dokument-Matrix) als Produkt kleinerer Matrizen angenähert darzustellen, und so den Rechenaufwand zu verkleinern. Dies ist dadurch möglich, dass das Verfahren eine Anzahl sogenannter Singulär¬werte für die Matrix liefert, und die vergleichsweise kleinen Singulärwerte vernachlässigt werden können.", file = paste( trainingsdir, "data6_base_04.txt", sep="/" ) )
write("Die Hoffnung bei der Anwendung von SVD für LSI ist nun, dass die grossen Singulärwerte, die dann übrigbleiben, die zugrundeliegende semantische Struktur genauer wiedergeben.", file = paste( trainingsdir, "data6_base_05.txt", sep="/" ) )
write("Von der benötigten Mathematik wird hier nichts bewiesen, weil dies keinen Sinn machen würde. Vielmehr wird von der verwendeten Methode soviel erklärt, wie nötig ist, um zu verstehen, warum dieses Verfahren eine Verbesserung bringen könnte.", file = paste( trainingsdir, "data6_base_06.txt", sep="/" ) )
write("Beim Information Retrieval (IR) werden einem Dokument Indexterme zugewiesen. Indexierung bedeutet also, den Inhalt eines Dokuments auf eine Menge relevanter Begriffe abzubilden. Eine Anfrage andererseits besteht aus Suchtermen, und ein Zugriff erfolgt, wenn die Suchterme in den Indextermen enthalten sind.", file = paste( trainingsdir, "data6_base_07.txt", sep="/" ) )
write("In der einfachsten Form wird eine Liste der für die Indexierung vorgesehenen Terme betrachtet (in der üblicherweise Terme, die in jedem Dokument vorkommen, weggelassen werden: sog. Stop-Listen) und für jedes Dokument ein Vektor, d.h. eine geordnete Liste von Zahlen erstellt, wobei jede Zahl für einen der Indexterme angibt, wie häufig er in diesem Dokument vorkommt.", file = paste( trainingsdir, "data6_base_08.txt", sep="/" ) )
write("Nur am Rande soll eine erste Verbesserungen in dieser Darstellung für ein Dokument erwähnt werden: Das TFIDK.  Die Abkürzung steht für „Term frequency inverse document frequency“, bei dem nicht die reine Termhäufigkeit im Dokument eingetragen wird, sondern die Häufigkeit des Terms in dem Dokument („Term frequency“) geteilt durch („inverse“) die Anzahl der Dokumente, welche den Term überhaupt enthalten („Document frequency“). Damit wird dem Umstand Rechnung getragen, dass Terme, die in sehr vielen der betrachteten Dokumente vorkommen, nicht so stark für das Retrieval gewichtet werden sollten wie andere, seltenere Terme.", file = paste( trainingsdir, "data6_base_09.txt", sep="/" ) )
write("Die Vektordarstellung, auf welche die Methode des LSI mit SVD führt, welche im Kapitel 4 beschrieben wird, unterscheidet sich grundsätzlich von solchen Term-Häufigkeits-Vektoren, denn die Einträge des Vektors gewichten nicht Indexterme, sondern aus der Term-Dokument-Matrix gewonnene, künstliche „semantische Konzepte“, welche im Gegensatz zu den meisten Indextermen wirklich unabhängige Faktoren darstellen.", file = paste( trainingsdir, "data6_base_10.txt", sep="/" ) )
write("Beim klassischen Dokumentenretrieval mit Suchtermen und Indextermen müssen die verwendeten Begriffe in dem Masse übereinstimmen, wie sie miteinander verglichen werden, und dies geschieht meist Buchstabenweise, allenfalls nach einer Stemmatisierung u.ä. Eins der Hauptprobleme für das IR ist jedoch die Tatsache, dass ein Wort verschiedene Typen von Objekten bezeichnen kann (Polysemie), sowie umgekehrt ein Objekt durch verschiedene Worte bezeichnet werden kann (Synonymie).", file = paste( trainingsdir, "data6_base_11.txt", sep="/" ) )
write("Meist sind nun die Personen, welche die Indexierung vornehmen, nicht dieselben Personen, welche die Anfragen formulieren, und benutzen daher oft nicht dieselben Terme, vor allem, wenn sich ihr Vorwissen und der Kontext, aus dem sie stammen, unterscheiden.", file = paste( trainingsdir, "data6_base_12.txt", sep="/" ) )
write("Ebenso werden meist nicht alle im Dokument verwendeten Terme zur Indexierung benutzt, weil wir sonst einerseits eine viel zu grosse Term-Dokument-Matrix erhalten würden, andererseits das Problem der Polysemie dadurch stärker ins Gewicht fällt.", file = paste( trainingsdir, "data6_base_13.txt", sep="/" ) )
write("Es sei an dieser Stelle bemerkt, dass die Methode des LSI mit SVD genau hier den Hebel ansetzt: Wir verwenden viele Terme zur Indexierung. Denn eine sehr grosse Matrix kann durch die SVD dennoch vernünftig gehandhabt werden, und es braucht viele Terme, um die semantische Struktur ausnutzen zu können und die Synonymie in den Griff zu bekommen.", file = paste( trainingsdir, "data6_base_14.txt", sep="/" ) )
write("Am Rande sei noch ein drittes Problem des IR erwähnt, welches jedoch erst bei der Bewertung von Treffern ins Spiel kommt: Die Redundanz. Üblicherweise wird ein Dokument als umso relevanter für eine Anfrage gezählt, je mehr der Suchterme in den Indextermen enthalten sind. Dabei werden redundante Terme jedoch gleich stark gewichtet wie unabhängige Terme. Auch dieses Problem will das hier vorgestellte Verfahren angehen: Die Suchterme werden vollständig durch unabhängige, künstliche Terme ersetzt. Geometrisch gesprochen werden „Durchschnitte“ von Termen verwendet, und Terme, die ähnliche Bedeutung haben, fallen weniger stark ins Gewicht, als solche mit stark unterschiedlicher Bedeutung.", file = paste( trainingsdir, "data6_base_15.txt", sep="/" ) )
write("Polysemie führt dazu, dass Dokumente Terme der Anfrage enthalten, jedoch mit anderer Bedeutung, so dass sie für die Anfrage gar nicht relevant sind. Somit erniedrigt sie die Precision. Synonymie führt dazu, dass Dokumente, die für die Anfrage relevant wären, nicht die Terme der Anfrage verwenden und somit nicht zurückgegeben werden. Sie erniedrigt den Recall.", file = paste( trainingsdir, "data6_base_16.txt", sep="/" ) )
write("Die reine Übereinstimmung von Suchtermen und Indextermen ist daher ein unzuverlässiges Kriterium für die Relevanz eines Dokuments für die Anfrage. Eine Verbesserung des Antwortverhaltens müsste auf konzeptueller Basis, dh. auf Bedeutungsebene geschehen.", file = paste( trainingsdir, "data6_base_17.txt", sep="/" ) )
write("Gemäss dem Thema des Seminars, soll dies nun Sprachdatenbasiert und automatisch, d.h. ohne jegliche menschliche Beurteilung der Semantik von Indizierungstermen geschehen.", file = paste( trainingsdir, "data6_base_18.txt", sep="/" ) )
write("Das Problem der Polysemie kann nur schwer automatisch angegangen werden. Die Anfrage müsste um Terme erweitert werden, welche die Bedeutung des polysemen Terms einschränken. Ansätze zur automatischen Behandlung der Synonymie finden sich z.B. in Synonymlisten, welche eine Anfrage automatisch um Terme erweitern, um vielleicht noch weitere relevante Dokumente zu finden. In beiden Fällen führen aber die zusätzlichen Terme selbst wieder zu weiteren unrelevanten Dokumenten, sozusagen in einen Teufelskreis.", file = paste( trainingsdir, "data6_base_19.txt", sep="/" ) )
write("Deerwester formuliert in seinem Artikel drei Anforderungen an eine Lösung der oben genannten Probleme. Von verschiedenen möglichen Ansätzen erfüllt nur das LSI mittels SVD alle drei Punkte. Ich möchte im Folgenden die drei Anforderungen formulieren und zu jeder kurz beschreiben, wieso Deerwester sie für nötig hält, und auf welche Weise seine vorgestellte Methode sie erfüllt. Dazu greife ich ein wenig auf das Kapitel 4 vor, denn die verwendeten Begriffe werden teilweise erst dort vorgestellt.", file = paste( trainingsdir, "data6_base_20.txt", sep="/" ) )
write("Das Hauptproblem, wie es im Kapitel 2 ausformuliert wurde, lässt sich in einem Wort als Unzuverlässigkeit der Daten zusammenfassen. Diese Unzuverlässigkeit ist aber bei jeder gegebenen Dokumentenbasis verschieden, und wir können von vornherein nichts über den Grad der Unzuverlässigkeit aussagen. Daher sollte eine Methode in irgendeiner Weise an das Problem angepasst werden können.", file = paste( trainingsdir, "data6_base_21.txt", sep="/" ) )
write("Bei LSI mit SVD ist es die Wahl von k (siehe Abschnitt 4.2)", file = paste( trainingsdir, "data6_base_22.txt", sep="/" ) )
write("Eine Anfrage wird als Pseudodokument, bestehend aus den Suchtermen angesehen. Um mittels dieser Terme zu den relevanten Dokumenten gelangen zu können, müssen die Terme in derselben Struktur liegen, wie die Dokumente. Dann können auch neue Dokumente leichter nachträglich hinzugefügt werden.", file = paste( trainingsdir, "data6_base_23.txt", sep="/" ) )
write("Es ist die mittlere, quadratische Matrix bei der SVD, die es ermöglicht, dass Terme und Dokumente bezüglich derselben Basis im gleichen semantischen Raum abgebildet werden können (siehe Abschnitt 4.1)", file = paste( trainingsdir, "data6_base_24.txt", sep="/" ) )
write("Da der semantische Raum nur mit sinnvoller Näherung dargestellt werden kann, wenn genügend Dimensionen modelliert werden, und  wie im Abschnitt 2.1. erwähnt, die Verwendung vieler Terme pro Dokument zur Lösung der Hauptprobleme des IR nötig sind, muss eine brauchbare Methode mit grossen Term-Dokument-Matrizen umgehen können.", file = paste( trainingsdir, "data6_base_25.txt", sep="/" ) )
write("Es ist genau der Hauptzweck der SVD, eine grosse Matrix so anzunähern, dass der Rechenaufwand in einem vernünftigen Rahmen bleibt.", file = paste( trainingsdir, "data6_base_26.txt", sep="/" ) )
write("Die Methode geht von der Hypothese aus, dass den Daten eine verborgene semantische Struktur zugrunde liegt, welche teilweise verborgen ist durch die Zufälligkeit der Wortwahl beim Retrieval, d.h. wir können von einem Term nicht mit Sicherheit auf die relevanten Dokumente schliessen. Für das Indizieren der Dokumente und das Retrieval wird bei LSI eine Beschreibung der Terme und Dokumente aufgrund dieser verborgenen semantischen Struktur verwendet.", file = paste( trainingsdir, "data6_base_27.txt", sep="/" ) )
write("Ausgegangen wird von einer genügend grossen Term-Dokument-Matrix. Wenn die Verwendung der Terme völlig unabhängig wäre, könnten wir aus diesen Daten keine Aussagen über die „wirklich“ relevanten Dokumente für gewisse Suchterme gewinnen. Jedoch gibt es sehr viele Zusammenhänge zwischen der Verwendung verschiedener Terme, sei es etwa, dass ein Term immer nur verwendet wird, wo auch ein anderer Term verwendet wird, oder sie sonst wie „ähnlich verwendet“ werden.", file = paste( trainingsdir, "data6_base_28.txt", sep="/" ) )
write("Aus einer grossen Term-Dokument-Matrix wird ein „semantischer Raum“ konstruiert, in welchem Terme und Dokumente, die stark miteinander zu tun haben, auch nahe beieinander liegen. Mit „semantisch“ ist hier gemeint, dass ein Term auf das Dokument selbst bzw. auf den Inhalt des Dokuments (wovon es handelt) Bezug nimmt, und das semantische Indizieren bedeutet, dem Dokument einen Punkt in diesem Raum zuzuordnen. ", file = paste( trainingsdir, "data6_base_29.txt", sep="/" ) )
write("Dies geschieht konkret mittels SVD, einer mathematischen Methode, welche automatisch durchgeführt werden kann. Das spezielle an SVD ist, dass die Methode auf einen Raum führt, der die grossräumigen assoziativen Muster in den Daten wiedergibt, aber die kleinen, weniger wichtigen Einflüsse ignoriert. Zum Beispiel können Terme, die gar nicht im Dokument vorkommen, nahe bei diesem Dokument landen, wenn dies dem zugrundeliegenden semantischen Muster der Daten entspricht. Mit anderen Worten, ein Dokument kann für einen Term, den es gar nicht enthält, als relevant erkannt werden, wenn z.B. viele mit dem Term assoziierten Terme vorkommen.", file = paste( trainingsdir, "data6_base_30.txt", sep="/" ) )
write("Die Strategie funktioniert besonders gut für Synonymie, und erhöht somit den Recall. Bei Polysemie ist sie nicht so erfolgreich. Dies kann man gut einsehen, wenn man sich vorstellt, wie der polyseme Term im semantischen Raum platziert werden muss: Seinen verschiedenen Bedeutungen sollen ja auch verschiedene Punkte im Raum entsprechen. Der Term selbst erhält aber nur einen Punkt, und der muss irgendwo zwischen diesen Punkten platziert werden. Wenn die verschiedenen Bedeutungen weit auseinanderliegen, wird der polyseme Term entsprechend schlecht platziert, weil er nicht gleichzeitig in der Nähe aller dieser Punkte liegen kann.", file = paste( trainingsdir, "data6_base_31.txt", sep="/" ) )
write("Ausgangspunkt der Methode ist die Menge der klassisch gebildeten Vektoren für jedes Dokument, d.h. jeder Eintrag bezeichnet einfach die jeweilige Häufigkeit des entsprechenden Terms in diesem Dokument. Stellen wir diese Vektoren nebeneinander, erhalten wir ein rechteckiges Schema von Zahlen, genannt Matrix. Dabei stehen in den Zeilen die Terme, in den Spalten die Dokumente:", file = paste( trainingsdir, "data6_base_32.txt", sep="/" ) )
write("Da wir wie weiter oben motiviert, eher viele der in den Dokumenten verwendeten Terme miteinbeziehen, wird die entstehende Term-Dokument-Matrix eher gross ausfallen. ", file = paste( trainingsdir, "data6_base_33.txt", sep="/" ) )
write("Bezeichnen wir die Anzahl der Terme mit t und die Anzahl der Dokumente mit d, so handelt es sich um d Vektoren der Länge t. Wären die Terme im geometrischen Sinn unabhängig, so hätten wir einen t-Dimensionalen Raum, in welchem die t Dimensionen jeweils einen der Terme darstellen würde. Wie wir oben gesehen haben, glauben wir aber eher, dass sie es nicht sind.", file = paste( trainingsdir, "data6_base_34.txt", sep="/" ) )
write("Der erste Vorteil der Singulärwertzerlegung ist nun, dass sie eine Menge von künstlichen Indextermen liefert, welche sicher unabhängig voneinander sind. Diese Zahl wird im allgemeinen kleiner sein als t, nennen wir sie m. Wir erhalten damit einen semantischen Raum der Dimension m, indem wir unsere Terme und Dokumente als Punkte darstellen können.", file = paste( trainingsdir, "data6_base_35.txt", sep="/" ) )
write("Jede Matrix X lässt sich als Produkt von drei Matrizen T, S, D schreiben, wobei S eine Diagonalmatrix ist (alle Werte ausserhalb der Diagonalen sind 0), und die Zeilen von T sowie die Spalten von D sind alle unabhängig, stehen nämlich senkrecht aufeinander.", file = paste( trainingsdir, "data6_base_36.txt", sep="/" ) )
write("Diese Darstellung heisst Singulärwertzerlegung der Matrix X. Ihre Existenz soll hier nicht bewiesen werden.", file = paste( trainingsdir, "data6_base_37.txt", sep="/" ) )
write("Da beim Multiplizieren von Matrizen Produkte der Einträge addiert werden, und die Reihenfolge bei der Addition egal ist, können Zeilen und Spalten der Matrix S vertauscht werden, solange auch die entsprechenden Spalten von T oder Zeilen von D auf dieselbe Weise vertauscht werden (solange also wieder dieselben Einträge miteinander multipliziert werden.)", file = paste( trainingsdir, "data6_base_38.txt", sep="/" ) )
write("Dadurch kann erreicht werden, dass die Werte auf der Diagonalen von S, welche Singulärwerte heissen, absteigend geordnet sind.", file = paste( trainingsdir, "data6_base_39.txt", sep="/" ) )
write("Was bringt uns diese Zerlegung? Nachdem die Singulärwerte absteigend geordnet wurden, befinden sich am unteren Ende der Diagonale vergleichsweise kleine Faktoren, welche vernachlässigt werden können. Wir wollen die Anzahl der Faktoren, welche wir für genügend gross und damit wichtig halten, k nennen, und alle andern ausser die ersten k Null setzen.  Diese Wahl von k ist völlig willkürlich und wird im nächsten Abschnitt diskutiert!", file = paste( trainingsdir, "data6_base_40.txt", sep="/" ) )
write("Wir erhalten so eine Näherung X’ für die Matrix X, welche sich als Produkt von kleineren Matrizen D’, S’ und T’ darstellen lässt. Denn die hinteren m-k Zeilen und die unteren m-k Spalten der Matrix S sind vollständig mit Nullen besetzt und können somit weggelassen werden, es entsteht S’. Ebenso können die hinteren m-k Spalten der Matrix T sowie die unteren m-k Zeilen der Matrix D gestrichen werden, da sie mit lauter Nullen multipliziert werden und somit nichts zum Resultat X’ beitragen. Es entstehen die kleineren Matrizen D’ bzw. T’.", file = paste( trainingsdir, "data6_base_41.txt", sep="/" ) )
write("An der Grösse der resultierenden Produktmatrix X’ hat sich hingegen nichts geändert, denn T’ enthält nach wie vor t Zeilen (d.h. weiterhin alle Terme) und D’ nach wie vor d Spalten (d.h. weiterhin alle Dokumente). Es gibt lediglich weniger zu Rechnen, wenn wir X durch X’ annähern. ", file = paste( trainingsdir, "data6_base_42.txt", sep="/" ) )
write("Betrachten wir indessen die Zeilen der Matrix T’ bzw. die Spalten der Matrix D’, so haben sie nur noch die Länge k, können also in einem Raum mit viel kleinerer Dimension k (<m) dargestellt werden!", file = paste( trainingsdir, "data6_base_43.txt", sep="/" ) )
write("Leider muss bei diesem Ersetzen der Term-Dokument-Matrix X durch eine Näherung X’ auch beachtet werden, dass das Retrieval nicht mehr auf dieselbe Weise durch Nachsehen der Einträge in X’ geschehen kann. Es müssen zusätzliche Regeln angegeben werden, wie das Retrieval nun zu geschehen hat, und die sind nicht ganz leicht zu verstehen. Trotzdem möchte ich kurz umreissen, wie dies von statten geht:", file = paste( trainingsdir, "data6_base_44.txt", sep="/" ) )
write("Kurz gesagt geht es darum, dass die gegenseitige Lage, genauer gesagt die Nähe von Punkten in dem semantischen Raum, welche Terme oder Dokumente darstellen können, direkt mit der Ähnlichkeit der Dokumente untereinander oder der Relevanz eines Terms für ein Dokument zu tun haben soll.", file = paste( trainingsdir, "data6_base_45.txt", sep="/" ) )
write("Als Mass für die Nähe gibt es zwei häufig verwendete Alternativen. Dazu ist zu beachten, dass von einem Punkt im Raum immer auch als Vektor, genauer gesagt seinem Ortsvektor, gesprochen werden kann, dazu werden die Koordinaten des Punktes einfach als Komponenten eines Vektors, der im Ursprung startet, angesehen. Dadurch ist es möglich, statt dem (euklidischen) Abstand zweier Punkte das Skalarprodukt der entsprechenden Vektoren zu betrachten, wie im folgenden kurz erklärt wird.", file = paste( trainingsdir, "data6_base_46.txt", sep="/" ) )
write("Der euklidische Abstand zweier Punkte ist definiert als die Länge des Vektors, welcher vom einen Punkt zum andern Punkt führt. Man bezeichnet die Formel für diesen Abstand oft als verallgemeinerten Pythagoras bezeichnet, weil für jede Dimension die Differenz der jeweiligen Koordinaten quadriert wird, und aus der Summe über alle Dimensionen die Wurzel gezogen wird: Abstand des Punkts A(a1, ... an) vom Punkt B(b1, ... bn):", file = paste( trainingsdir, "data6_base_47.txt", sep="/" ) )
write("Im zweidimensionalen Fall, für n=2 entspricht dies der Hypotenuse eines Rechtwinkligen Dreiecks, im dreidimensionalen Fall  (n=3) der Raumdiagonale eines Quaders.", file = paste( trainingsdir, "data6_base_48.txt", sep="/" ) )
write("Das Skalarprodukt (auch inneres Produkt oder engl. Dot-Product) zweier Vektoren ist propor¬tional zum Cosinus des Winkels zwischen den beiden Vektoren. Das Skalarprodukt wird meist vorgezogen, weil es sich viel einfacher berechnen lässt: die jeweiligen Koordinaten werden multipliziert und die Produkte für alle Dimensionen addiert (genau wie beim Multiplizieren von Matrizen). Es ist nur proportional zum Cosinus des Zwischenwinkels, weil man das Skalarprodukt noch durch das Produkt der Längen der beiden Vektoren teilen müsste.", file = paste( trainingsdir, "data6_base_49.txt", sep="/" ) )
write("Skalarprodukt der Vektoren und :", file = paste( trainingsdir, "data6_base_50.txt", sep="/" ) )
write("Cosinus des Zwischenwinkels:", file = paste( trainingsdir, "data6_base_51.txt", sep="/" ) )
write("Was nun bei der Singulärwertzerlegung diese Distanzbestimmung etwas erschwert, ist der Umstand, dass wir, wenn wir als Basis für unseren k-dimensionalen semantischen Raum die k künstlichen Konzepte, welche gerade den Zeilen (die identisch sind mit den Spalten) der quadratischen Diagonal-Matrix S entsprechen, verwenden, alle Vektoren auf die Länge 1 normiert haben. Um wahre Aussagen über Entfernungen dieser Punkte voneinander zu erhalten, müssen wir jeweils wieder mit den Singulärwerten aus S gewichten und somit die entsprechende Skalierung wieder rückgängig machen.", file = paste( trainingsdir, "data6_base_52.txt", sep="/" ) )
write("Das wichtigste Beispiel soll hier konkret angegeben werden. Eine Anfrage (A) wird als Pseudodokument im Raum platziert. Wo kommt sie zu liegen? Als neues Dokument würde sie ja eigentlich der Term-Dokument-Matrix X’ eine zusätzliche Spalte anfügen, wir nennen diese Spalte xA. Wir wissen, wie diese Spalte aussieht: Die Einträge sind einfach die Häufigkeiten der entsprechenden Terme. In der Zerlegung wäre es eine zusätzliche Spalte der Matrix D’, genannt dA, und diese suchen wir. Durch Umformen der Gleichung X’ = T’S’D’ ergibt sich:", file = paste( trainingsdir, "data6_base_53.txt", sep="/" ) )
write("Ähnlich sieht es beim Vergleich von Termen und Dokumenten untereinander aus oder bei der Bestimmung der Relevanz eines Terms für ein Dokument: Dokumente müssen durch Multiplikation von S, Terme von S-1 für den Vergleich untereinander gewichtet werden, und für Term-Dokument-Vergleiche beide mit S1/2 (wo jeweils die Wurzel aus den Singulärwerten gezogen wurde).", file = paste( trainingsdir, "data6_base_54.txt", sep="/" ) )
write("dA = xAT’S’-1		(S’-1 bezeichnet die Inverse Matrix von S’)", file = paste( trainingsdir, "data6_base_55.txt", sep="/" ) )
write("Wie wir gesehen haben, kann der Grad der Dimensionsreduktion frei gewählt werden. Je kleiner wir k wählen, desto ungenauer wird zwar die Term-Dokument-Matrix angenähert, aber desto  kleiner sind die Matrizen und somit auch der Rechenaufwand.", file = paste( trainingsdir, "data6_base_56.txt", sep="/" ) )
write("Neben dieser rein technischen Motivation gilt es aber noch die Hypothese des zugrunde¬liegenden semantischen Raumes und die Unzuverlässigkeit der Daten zu beachten: Wir wollen den Raum nämlich gar nicht exakt wiedergeben, sondern nur vernünftig. Es besteht nun nämlich die Hoffnung, dass die grossen Singulärwerte gerade die wichtigen und zuverlässigen Strukturen der Daten modellieren, und die kleinen Details eher Störungen wie z.B. Redundanz darstellen, die wir lieber loswerden wollen.", file = paste( trainingsdir, "data6_base_57.txt", sep="/" ) )
write("Andererseits können reale konzeptuelle Beziehungen komplexe Strukturen wie lokale Hierarchien und Interaktionen zwischen Bedeutungen enthalten, die eine gewisse Dimension benötigen, um sinnvoll angenähert werden zu können. Eine Dimension, die dafür zu klein ist, ist somit nicht wünschenswert, aber wird auch gar nicht angestrebt, da wir die semantische Struktur nicht interpretieren wollen, auch nicht geometrisch visualisieren, verstehen oder die neuen Konzepte verbalisieren.", file = paste( trainingsdir, "data6_base_58.txt", sep="/" ) )
write("Somit bestehen sowohl Gründe, den Wert von k eher gross, als auch, ihn eher klein zu wählen. Die Vor- und Nachteile jeder Wahl sind in der folgenden Tabelle zusammengefasst:", file = paste( trainingsdir, "data6_base_59.txt", sep="/" ) )
write("Wert von k: niedrig hoch Vorteile : schneller exaktere Näherung (Motivation)			nur Wichtiges Nachteile:			zu ungenau für			langsamer (wenn zu extrem)		komplexe Strukturen		störende Details", file = paste( trainingsdir, "data6_base_60.txt", sep="/" ) )
write("Im Gegensatz zu vielen Anwendungen der SVD, bei denen der Wert durch Optimieren der Rechengeschwindigkeit bei maximal tolerierbarer Ungenauigkeit ermittelt wird, ist für das LSI das alleinige Kriterium die resultierende Retrievaleffektivität!", file = paste( trainingsdir, "data6_base_61.txt", sep="/" ) )
write("Es gibt keine allgemeingültige Aussage über die richtige Wahl von k, der Wert muss für jede Datenbasis empirisch ermittelt werden. Dies war jedoch auch eine Anforderung an die Methode, dass sie sich an die jeweils vorliegende Unzuverlässigkeit anpassen lässt, siehe Kapitel 3.", file = paste( trainingsdir, "data6_base_62.txt", sep="/" ) )
write("Eine Anfrage wird als „Pseudo-Dokument“ im Schwerpunkt der gewichteten enthaltenen Terme als neuer Punkt im semantischen Raum platziert, ist also charakterisiert durch einen Vektor der Länge k von „Gewichten“, welche die Stärke der Assoziation mit jedem dieser zugrundeliegenden künstlichen Konzepten anzeigt. Die Dokumente in der Nähe werden zurückgegeben, wobei das oben beschriebene Cosinus-Mass verwendet wird.", file = paste( trainingsdir, "data6_base_63.txt", sep="/" ) )
write("Ein Hauptproblem des IR ist die nicht eindeutige Term-Objekt-Relation wegen Synonymie und Polysemie. Auf der andern Seite sind natürliche Indexterme, sobald sie eine vernünftige Anzahl erreichen, nicht unabhängig sondern in irgendeiner Form korreliert.", file = paste( trainingsdir, "data6_base_64.txt", sep="/" ) )
write("LSI durch SVD kann den Umgang mit Synonymie verbessern, indem es die unzuverlässigen individuelle Terme der Anfrage werden durch voneinander unabhängige, künstliche Konzepte ersetzt. Diese bilden eine Basis für einen „semantischen Raum“, welcher der Verwendung der Terme in den Dokumenten zugrunde liegt.", file = paste( trainingsdir, "data6_base_65.txt", sep="/" ) )
write("Die Dimension dieses Raumes kann nach Belieben verkleinert werden, und es besteht die Hoffnung, dass dadurch die wichtigen und zuverlässigen semantischen Strukturen stärker zutage treten.", file = paste( trainingsdir, "data6_base_66.txt", sep="/" ) )
write("Die Methode liefert ein Mass für die Ähnlichkeit von Dokumenten oder Termen untereinander sowie die Relevanz eines Terms für ein Dokument.", file = paste( trainingsdir, "data6_base_67.txt", sep="/" ) )
write("Der modellierte semantische Raum enthält Terme und Dokumente zugleich, sodass eine Anfrage als Pseudodokument im Schwerpunkt der Suchterme in diesem Raum platziert werden kann, und Dokumente in der Nähe dieses Punktes werden zurückgegeben.", file = paste( trainingsdir, "data6_base_68.txt", sep="/" ) )
write("Dadurch können auch Dokumente gefunden werden, welche die Suchterme gar nicht enthalten.", file = paste( trainingsdir, "data6_base_69.txt", sep="/" ) )
write("LSI (Latent Semantic Indexing)		Indexierung benutzt die zugrundeliegende Semantische Struktur", file = paste( trainingsdir, "data6_base_70.txt", sep="/" ) )
write("SVD (Singular Value Decomposition)	Term-Dokument-Matrix wird als Produkt von speziellen, kleineren Matrizen angenähert dargestellt.", file = paste( trainingsdir, "data6_base_71.txt", sep="/" ) )

write("Schritt 1: Repräsentation der Texte in einer Matrix. Die Zeilen der Matrix stellen die einzelnen Wörtern und die Spalten einen Absatz oder eine andere Zusammenhang dar. Jede Zelle beinhaltet die Häufigkeit mit welcher ein Wort  seiner Zeile im von seiner Spalte angegebenen Absatz vorkommt. Die Zelleneinträge unterliegen eine Vortransformation, in welchen die Häufigkeit der Zellen durch eine Funktion gewichtet ist. Diese Funktion drückt gleichzeitig die Wichtigkeit des Wortes in den einzelnen Absätzen sowie den Grad, zu welchem das Worttyp Informationen im Allgemeinen enthalten ist, aus. Schritt 2: Nächstens werden die Daten mit einem mathematischen Verfahren (Faktorisierung durch Singular Value Decomposition) bearbeitet, wobei die Datenmenge reduziert wird und wodurch die in den Texten verborgene semantische Struktur aufgedeckt werden soll. Durch Zerlegung und Umwandlung wird die optimale Dimension erreicht, welche die korrekte Induktion der zugrundeliegende Relationen bestimmt. Schritt 3: Dokumente und Worte werden als Vektoren im gleichen multidimensionalen Vektorraum dargestellt. Der Kosinus zwischen den Vektoren ist üblicherweise (aber nicht immer) der Maß der Ähnlichkeiten zwischen der zwei Texten.", file = paste( trainingsdir, "data6_golden_01.txt", sep="/" ) )
write("LSA ist eine Methode, die versucht Kontext-Zusammenhänge von unterschiedlichen Texten zu finden und zu beschreiben. Die Technik geht nach folgenden Schritten vor. Zu Beginn wird der zu untersuchende Text als Matrix dargestellt, wobei jede Reihe ein Wort, und jede Spalte eine Textpassage repräsentiert. Im nächsten Schritt werden die einzelnen Zellen transformiert. Dabei wird die Zellenhäufigkeit mit einer Funktion gewichtet die einerseits die Relevanz eines Wortes in der jeweiligen Textpassage und andererseits den generellen Informationsgehalt des Wortes widerspiegelt. Danach wird eine \"Einzelwertzerlegung\", eine Form der Faktoranalyse, an der Matrix angewandt. Dabei wird die rechteckige Matrix in drei neue Matrizen zerlegt. Die erste der drei Komponentenmatrizen beschreibt die Zeilenwerte der Originalmatrix als Vektoren des rechteckigen Faktorwertes, die zweite beschreibt die originalen Spaltenwerte und die dritte ist eine Diagonalmatrix, welche die abgestuften Werte enthält. Die Originalmatrix ist das Produkt der drei Matrizen. Durch Multiplizieren der drei neuen Matrizen kann also die Originalmatrix wiederhergestellt werden. Werden nicht alle Faktoren verwendet, zeigt die wiederhergestellte Matrix die beste Passung des kleinsten Quadrats (least-squares best fit). Die Lösung kann ganz einfach reduziert werden, indem Faktoren in der Diagonalmatrix gelöscht werden. Üblicherweise wird dabei mit dem kleinsten Faktor begonnen. Um die Auswirkung der Reduktion von Dimensionen auf die Relation zwischen den Faktoren zu untersuchen, wird die Interkorrelation zwischen den einzelnen Faktoren berechnet. Aufgrund dieser Interkorrelationen kann auf die Kontextzusammenhänge der einzelnen Faktoren geschlossen werden.", file = paste( trainingsdir, "data6_golden_02.txt", sep="/" ) )
write("Mit Hilfe der Latent Semantic Analysis wird versucht, semantisch äquivalente, aber unterschiedlich geschriebene Worte zu identifizieren, und zwar durch mathematische Methoden. Zuerst wird vom vorgegebenen Text eine Dokument/Wort Matrix erstellt, in die jedes einzelne Wort der Textpassage in Zusammenhang mit der Häufigkeit des Auftretens innerhalb des Textes/der Textstelle gebracht wird. Es wird weiters ein Wert errechnet, der sowohl nach der Relevanz des Wortes in dem untersuchten Abschnitt gewichtet ist, als auch nach dem Grad des Informationsgehaltes unabhängig von dem Text. Durch die folgende Singular Value Decomposition (SVD) wird eine Faktoranalyse durchgeführt und die Dokument/Wort Matrix in drei Matrizen auf gespalten, die erstens die originalen Zeileneinträge als Vektoren der orthogonalen Faktorwerte, zweitens die Spalteneinträge auf selbige Art und drittens (Diagonale) die singular values, die Skalenwerte enthalten, welche durch Multiplikation wieder die originären Matrixwerte ergeben. Nun werden die Werte neu sortiert und ausgerichtet, die Faktoren werden reduziert, sodass die Matrix in einem weniger dimensionalen Raum projezierbar ist. Nun können zwei Worte verglichen werden, indem die Entfernung der jeweiligen Wortvektoren innerhalb der reduzierten Matrix ermittelt wird und man kann auf diese Weise Rückschlüsse auf die semantische Bedeutung der Worte ziehen.", file = paste( trainingsdir, "data6_golden_03.txt", sep="/" ) )
